# DATViL: Dual Adapter Tuning of Vision–Language Models Using Large Language Models

This is the official implementation of **“Dual Adapter Tuning of Vision–Language Models Using Large Language Models”**, published in *International Journal of Computational Intelligence Systems* (2025).

[![DOI](https://img.shields.io/static/v1?label=DOI&message=10.1007%2Fs44196-025-00853-0&color=blue)](https://doi.org/10.1007/s44196-025-00853-0)
[![Python](https://img.shields.io/badge/python-3.7%2B-blue)](https://www.python.org/)
[![PyTorch](https://img.shields.io/badge/pytorch-1.8%2B-orange)](https://pytorch.org/)
[![License](https://img.shields.io/badge/License-MIT-green)](LICENSE)


## Overview

Recent advances in vision–language models (VLMs), like CLIP, have enabled strong performance in zero-shot and few-shot image recognition. However, most efficient transfer learning (ETL) approaches only tune either prior-dependent or prior-independent adapters, and often use generic, template-based text prompts.

**DATViL** introduces a novel ETL framework that:
1. **Dual Adapter Tuning:** Simultaneously leverages both prior-dependent and prior-independent adapters for VLMs, combining the strengths of both.
2. **LLM-Generated Prompts:** Instead of using generic templates, DATViL utilizes a pre-trained LLM (GPT-4) to automatically generate attribute-specific prompts for each visual class, and context-aware discriminative descriptions for top candidate classes.
3. **Context-Aware Reasoning:** Further guides the VLM with context-aware descriptions to improve recognition between most probable candidate classes.

DATViL achieves state-of-the-art results on 11 diverse benchmarks across multiple few-shot settings.


## Installation

1. **Clone the repository**
    ```bash
    git clone https://github.com/mrzarei5/DATViL.git
    cd DATViL
    ```

2. **Create and activate a Conda environment**
    ```bash
    conda create -n datvil python=3.7
    conda activate datvil
    ```

3. **Install dependencies**
    ```bash
    pip install -r requirements.txt
    # For GPU support (adjust CUDA version as needed)
    pip install torch torchvision --index-url https://download.pytorch.org/whl/cu118
    ```

## Data Preparation

- **Dataset**

    Follow [this link](https://github.com/KaiyangZhou/CoOp/blob/main/DATASETS.md) to install all datasets.

- **Class-specific Attributes and Context-aware Descriptions**

    All class-specific attributes and context-aware descriptions generated by GPT-4 are available in the folders "attributes_gpt4" and "attributes_discriminative_gpt4". Copy these folders to the datasets root directory.

## How to Run

### 1. Run DATViL-C (attribute-specific dual adapter tuning)
```bash
python main.py --config configs/YOUR_DATASET.yaml --model_name datvilc --root_path ./datasets
```

- Replace "root_path" with the path to the root folder of all datasets.


### 2. Update context-aware descriptions for candidate classes

Before running DATViL, ensure that context-aware descriptions are up-to-date for each test sample:

```bash
python extract_gpt4.py --labels_file dataset_labels.json --description_file dataset.json --dataset YOUR_DATASET
```
    
- Replace `labels_file` with the path to the labels file saved by DATViL-C.
- `description_file` is the path to the provided context-aware descriptions in the folder "attributes_discriminative_gpt4".
- Replace `dataset` with the dataset name.

**Note**: Set your OpenAI key in "extract_gpt4.py".

### 3. Run DATViL (full dual adapter tuning + context-aware descriptions)

```bash
python main.py --config configs/YOUR_DATASET.yaml --model_name datvil --root_path ./datasets
```
- Replace `root_path` with the path to the root folder of all datasets.

**Additional Settings**

For running DATViL-C and DATViL, adjust the training settings using the following arguments:

- `"shots"`: Number of shots used for training.
- `"per_sample_train"`: Number of per-sample training epochs in DATViL.

Alpha and beta values, backbone type, and learning rates can be changed in dataset config files available in `./configs/`.

## Citation

If you use this work, please cite:
```bibtex
@article{Zarei2025DATViL,
    title   = {Dual Adapter Tuning of Vision–Language Models Using Large Language Models},
    author  = {Mohammad Reza Zarei and Abbas Akkasi and Majid Komeili},
    journal = {International Journal of Computational Intelligence Systems},
    volume  = {18},
    pages   = {109},
    year    = {2025},
    doi     = {10.1007/s44196-025-00853-0},
    url     = {https://doi.org/10.1007/s44196-025-00853-0}
}
```